{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tuto_dataset import *\n",
    "from utils import *\n",
    "from model_GCN import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameter ======================================\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "epochs = 20\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/state_q.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    state = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = state\n",
    "for i in range(len(state)):\n",
    "    for j in range(len(state[0])):\n",
    "        data_x[i][j] = np.float32(state[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = list()\n",
    "with open('./data/all_users_stands.txt', 'r') as f:\n",
    "    for i in range(1563):\n",
    "        stand = f.readline()\n",
    "        stand = stand.replace('\\n', '')\n",
    "        data_y.append(stand)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "# opponent label = 0, proponent label = 1\n",
    "for i in range(1563):\n",
    "    data_y[i] = int(data_y[i])\n",
    "    if data_y[i] == -1:\n",
    "        data_y[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read adj_dict\n",
    "a_file = open(\"adj_dict.pkl\", \"rb\")\n",
    "adj_dict = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adjacency(adj_dict):\n",
    "\n",
    "    edge_index = []\n",
    "    num_nodes = len(adj_dict)\n",
    "    for src, dst, in adj_dict.items():\n",
    "        edge_index.extend([src, v] for v in dst)\n",
    "        edge_index.extend([v, src] for v in dst)\n",
    "    \n",
    "    # removed replicated edges\n",
    "    edge_index = list(k for k, _ in itertools.groupby(sorted(edge_index)))\n",
    "    edge_index = np.asarray(edge_index)\n",
    "        \n",
    "    # A sparse matrix in COOrdinate format.\n",
    "    adjacency = sp.coo_matrix((np.ones(len(edge_index)), (edge_index[:,0], edge_index[:,1])), shape=(num_nodes, num_nodes), dtype=\"float32\")\n",
    "\n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a sparse matrix based on the adjacency dict\n",
    "adj = build_adjacency(adj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1439,    2, 1340, ..., 1546, 1546, 1546], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "adj.tocoo().col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select train data, test data and pred data. Create masks.\n",
    "num_label = 180\n",
    "num_total = 1563\n",
    "num_nodes = 1563\n",
    "num_size = 512 # the length of a word vector\n",
    "\n",
    "# l have 180 labelled data. Among those data, 160 are used to train the model and the rest 20 labeled data are used to test the model.\n",
    "# then we shuffle the list, which select train data in a random way\n",
    "train_test_mask = [True]*(num_label-20) + [False]*(20) \n",
    "\n",
    "random.shuffle(train_test_mask)\n",
    "train_mask = train_test_mask + [False]*(num_total - num_label)\n",
    "test_mask = [not stand for stand in train_test_mask] + [False]*(num_total - num_label)\n",
    "pred_mask = [False]*num_label + [True]*(num_total - num_label)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Analyzing data ...\n------------------------------------------------------\nNode's feature shape:  (1563, 512)\nNode's label shape:  (1563,)\nAdjacency's shape:  (1563, 1563)\nNumber of train nodes:  160\nNumber of validation nodes:  20\nNumber of test nodes:  1383\n------------------------------------------------------\n"
    }
   ],
   "source": [
    "# loading data ============================================= \n",
    "# convert all list objects to ndarray type\n",
    "data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "train_mask = np.array(train_mask)\n",
    "test_mask = np.array(test_mask)\n",
    "pred_mask = np.array(pred_mask)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_X = scaler.fit_transform(data_x)\n",
    "x = normalize(scaled_X, norm='l1', axis=1, copy=True)\n",
    "\n",
    "#x = data_x / data_x.sum(1, keepdims=True)\n",
    "tensor_x = torch.from_numpy(x).to(device)\n",
    "tensor_x = torch.tensor(np.float32(tensor_x))\n",
    "\n",
    "tensor_y = torch.from_numpy(data_y).to(device)\n",
    "\n",
    "tensor_train_mask = torch.from_numpy(train_mask).to(device)\n",
    "tensor_test_mask = torch.from_numpy(test_mask).to(device)\n",
    "tensor_pred_mask = torch.from_numpy(pred_mask).to(device)\n",
    "\n",
    "normalize_adjacency = normalization(adj) \n",
    "\n",
    "indices = torch.from_numpy(np.asarray([normalize_adjacency.row, normalize_adjacency.col]).astype('int64')).long()\n",
    "values = torch.from_numpy(normalize_adjacency.data.astype(np.float32))\n",
    "tensor_adjacency = torch.sparse.FloatTensor(indices, values, (num_nodes, num_nodes)).to(device)\n",
    "print('Analyzing data ...')\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Node's feature shape: \", data_x.shape)\n",
    "print(\"Node's label shape: \", data_y.shape)\n",
    "print(\"Adjacency's shape: \", adj.shape)\n",
    "print(\"Number of train nodes: \", train_mask.sum())\n",
    "print(\"Number of validation nodes: \", test_mask.sum())\n",
    "print(\"Number of test nodes: \", pred_mask.sum())\n",
    "print(\"------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mask):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_adjacency, tensor_x)\n",
    "        test_mask_logits = logits[mask]\n",
    "        predict_y = test_mask_logits.max(1)[1]\n",
    "        accuracy = torch.eq(predict_y, tensor_y[mask]).float().mean()\n",
    "\n",
    "    return accuracy, predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 000: Loss 0.665601, Train Acc 0.8938, Val Acc 0.9000\nEpoch 001: Loss 0.444660, Train Acc 0.8938, Val Acc 0.9000\nEpoch 002: Loss 0.353434, Train Acc 0.8938, Val Acc 0.9000\nEpoch 003: Loss 0.356554, Train Acc 0.8938, Val Acc 0.9000\nEpoch 004: Loss 0.383135, Train Acc 0.8938, Val Acc 0.9000\nEpoch 005: Loss 0.393604, Train Acc 0.8938, Val Acc 0.9000\nEpoch 006: Loss 0.385746, Train Acc 0.8938, Val Acc 0.9000\nEpoch 007: Loss 0.367693, Train Acc 0.8938, Val Acc 0.9000\nEpoch 008: Loss 0.348777, Train Acc 0.8938, Val Acc 0.9000\nEpoch 009: Loss 0.337199, Train Acc 0.8938, Val Acc 0.9000\nEpoch 010: Loss 0.337453, Train Acc 0.8938, Val Acc 0.9000\nEpoch 011: Loss 0.345650, Train Acc 0.8938, Val Acc 0.9000\nEpoch 012: Loss 0.350761, Train Acc 0.8938, Val Acc 0.9000\nEpoch 013: Loss 0.347586, Train Acc 0.8938, Val Acc 0.9000\nEpoch 014: Loss 0.339284, Train Acc 0.8938, Val Acc 0.9000\nEpoch 015: Loss 0.331394, Train Acc 0.8938, Val Acc 0.9000\nEpoch 016: Loss 0.327184, Train Acc 0.8938, Val Acc 0.9000\nEpoch 017: Loss 0.326766, Train Acc 0.8938, Val Acc 0.9000\nEpoch 018: Loss 0.328079, Train Acc 0.8938, Val Acc 0.9000\nEpoch 019: Loss 0.329000, Train Acc 0.8938, Val Acc 0.9000\n"
    }
   ],
   "source": [
    "# build model =================================================\n",
    "from model_GCN import *\n",
    "\n",
    "model = GCN_Network(num_size).to(device)\n",
    "\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "#def train():\n",
    "\n",
    "loss_history = []\n",
    "val_acc_history = []\n",
    "model.train()\n",
    "train_y = tensor_y[tensor_train_mask]\n",
    "train_y = torch.tensor(np.float32(train_y))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logits = model(tensor_adjacency, tensor_x)\n",
    "    train_mask_logits = logits[tensor_train_mask]\n",
    "    train_mask_logits = train_mask_logits.squeeze(1)\n",
    "    loss = criterion(train_mask_logits, train_y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc, _ = test(tensor_train_mask)\n",
    "    val_acc, _ = test(tensor_test_mask)\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "    val_acc_history.append(val_acc.item())\n",
    "\n",
    "    print(\"Epoch {:03d}: Loss {:04f}, Train Acc {:.04f}, Val Acc {:.04f}\".format(epoch, loss.item(), train_acc.item(), val_acc.item()))\n",
    "\n",
    "#    return loss_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.0911, 0.0552, 0.0472, 0.0923, 0.0929, 0.0776, 0.0917, 0.0729, 0.0911,\n        0.0249, 0.0839, 0.0905, 0.0871, 0.0476, 0.0905, 0.0511, 0.0849, 0.0954,\n        0.0919, 0.0541, 0.0851, 0.0923, 0.0910, 0.0731, 0.0909, 0.0924, 0.0690,\n        0.0746, 0.0778, 0.0852, 0.0802, 0.0874, 0.0901, 0.0909, 0.0699, 0.0738,\n        0.0482, 0.0911, 0.0928, 0.0786, 0.0911, 0.0403, 0.0913, 0.0911, 0.0928,\n        0.0906, 0.0946, 0.0767, 0.0635, 0.0938, 0.0295, 0.0910, 0.0635, 0.0904,\n        0.0339, 0.0775, 0.0903, 0.0166, 0.0839, 0.0800, 0.0268, 0.0912, 0.0926,\n        0.0688, 0.0832, 0.0697, 0.0930, 0.0717, 0.0928, 0.0918, 0.0913, 0.0483,\n        0.0929, 0.0054, 0.0910, 0.0903, 0.0240, 0.0776, 0.0902, 0.0907, 0.0768,\n        0.0956, 0.0930, 0.0644, 0.0907, 0.0841, 0.0989, 0.0735, 0.0006, 0.0742,\n        0.0366, 0.0916, 0.0795, 0.0336, 0.0681, 0.0910, 0.0656, 0.0959, 0.0947,\n        0.0925, 0.0847, 0.0894, 0.0843, 0.0924, 0.0911, 0.0912, 0.0849, 0.0904,\n        0.0785, 0.0848, 0.0669, 0.0902, 0.0918, 0.0880, 0.0674, 0.0933, 0.0351,\n        0.0905, 0.0767, 0.0732, 0.0581, 0.0894, 0.0674, 0.0938, 0.0723, 0.0751,\n        0.0589, 0.0855, 0.0905, 0.0947, 0.0775, 0.0949, 0.0776, 0.0422, 0.0908,\n        0.0694, 0.0549, 0.0813, 0.0930, 0.0722, 0.0828, 0.0802, 0.0943, 0.0901,\n        0.0888, 0.0879, 0.0945, 0.0876, 0.0979, 0.0945, 0.0945, 0.0979, 0.0906,\n        0.0964, 0.0889, 0.0945, 0.0979, 0.0945, 0.0945, 0.0979],\n       grad_fn=<SqueezeBackward1>)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train_mask_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# PREDICTION\n",
    "# ==========================================================\n",
    "_, prediction = test(tensor_pred_mask)\n",
    "# print(\"Testing Acc {:.4}\".format(accuracy))\n",
    "prediction.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.17-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}