{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "from tensorflow.python.framework import constant_op\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from itertools import chain\n",
    "import os\n",
    "import math\n",
    "#from model import LSTMDSSM, _START_VOCAB\n",
    "import csv\n",
    "\n",
    "random.seed(1229)\n",
    "tf.flags.DEFINE_string('f', '', 'kernel')\n",
    "tf.flags.DEFINE_boolean(\"is_train\", True, \"Set to False to inference.\")\n",
    "tf.flags.DEFINE_boolean(\"read_graph\", False, \"Set to False to build graph.\")\n",
    "tf.flags.DEFINE_integer(\"symbols\", 400000, \"vocabulary size.\")\n",
    "tf.flags.DEFINE_integer(\"epoch\", 25, \"Number of epoch.\")\n",
    "tf.flags.DEFINE_integer(\"embed_units\", 300, \"Size of word embedding.\")\n",
    "tf.flags.DEFINE_integer(\"units\", 512, \"Size of each model layer.\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 20, \"Batch size to use during training.\")\n",
    "tf.flags.DEFINE_string(\"data_dir\", \"./data\", \"Data directory\")\n",
    "tf.flags.DEFINE_string(\"train_dir\", \"./data\", \"Training directory.\")\n",
    "tf.flags.DEFINE_boolean(\"log_parameters\", True, \"Set to True to show the parameters\")\n",
    "tf.flags.DEFINE_string(\"time_log_path\", 'time_log.txt', \"record training time\")\n",
    "tf.flags.DEFINE_integer(\"neg_num\", 1, \"negative sample number\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SimpleLSTMCell(tf.contrib.rnn.RNNCell):\n",
    "    \"\"\"\n",
    "    The simpler version of LSTM cell with forget gate set to 1, according to the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, forget_bias=1.0, activation=tf.tanh, reuse=None):\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._activation = activation\n",
    "        self._reuse = reuse\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return (self._num_units, self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        with tf.variable_scope(scope or \"simple_lstm_cell\", reuse=self._reuse):\n",
    "            c, h = state\n",
    "            if not hasattr(self, '_wi'):\n",
    "                self._wi = tf.get_variable('simple_lstm_cell_wi', dtype=tf.float32, shape=[inputs.get_shape()[-1] + h.get_shape()[-1], self._num_units], initializer=tf.orthogonal_initializer())\n",
    "                self._bi = tf.get_variable('simple_lstm_cell_bi', dtype=tf.float32, shape=[self._num_units], initializer=tf.constant_initializer(0.0))\n",
    "                self._wo = tf.get_variable('simple_lstm_cell_wo', dtype=tf.float32, shape=[inputs.get_shape()[-1] + h.get_shape()[-1], self._num_units], initializer=tf.orthogonal_initializer())\n",
    "                self._bo = tf.get_variable('simple_lstm_cell_bo', dtype=tf.float32, shape=[self._num_units], initializer=tf.constant_initializer(0.0))\n",
    "                self._wc = tf.get_variable('simple_lstm_cell_wc', dtype=tf.float32, shape=[inputs.get_shape()[-1] + h.get_shape()[-1], self._num_units], initializer=tf.orthogonal_initializer())\n",
    "                self._bc = tf.get_variable('simple_lstm_cell_bc', dtype=tf.float32, shape=[self._num_units], initializer=tf.constant_initializer(0.0))\n",
    "            i = tf.nn.sigmoid(tf.matmul(tf.concat([inputs, h], 1), self._wi) + self._bi)\n",
    "            o = tf.nn.sigmoid(tf.matmul(tf.concat([inputs, h], 1), self._wo) + self._bo)\n",
    "            _c = self._activation(tf.matmul(tf.concat([inputs, h], 1), self._wc) + self._bc)\n",
    "            # remove forget gate according to the paper\n",
    "            new_c = c + i * _c\n",
    "            new_h = o * self._activation(new_c)\n",
    "\n",
    "            return new_h, (new_c, new_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.ops.nn import dynamic_rnn\n",
    "from tensorflow.contrib.lookup.lookup_ops import MutableHashTable\n",
    "import math\n",
    "\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "_START_VOCAB = ['_PAD', '_UNK']\n",
    "\n",
    "\n",
    "class LSTMDSSM(object):\n",
    "    \"\"\"\n",
    "    The LSTM-DSSM model refering to the paper: Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval.\n",
    "    papaer available at: https://arxiv.org/abs/1502.06922\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_lstm_units,\n",
    "                 embed,\n",
    "                 neg_num=1,\n",
    "                 gradient_clip_threshold=5.0):\n",
    "\n",
    "        self.queries = tf.placeholder(dtype=tf.string, shape=[None, None])  # shape: batch*len\n",
    "        self.queries_length = tf.placeholder(dtype=tf.int32, shape=[None])  # shape: batch\n",
    "        self.docs = tf.placeholder(dtype=tf.string, shape=[neg_num + 1, None, None])  # shape: (neg_num + 1)*batch*len\n",
    "        self.docs_length = tf.placeholder(dtype=tf.int32, shape=[neg_num + 1, None])  # shape: batch*(neg_num + 1)\n",
    "\n",
    "        self.word2index = MutableHashTable(\n",
    "            key_dtype=tf.string,\n",
    "            value_dtype=tf.int64,\n",
    "            default_value=UNK_ID,\n",
    "            #shared_name=\"in_table\",\n",
    "            name=\"in_table\",\n",
    "            checkpoint=True\n",
    "        )\n",
    "\n",
    "        self.learning_rate = tf.Variable(0.003, trainable=False, dtype=tf.float32)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.epoch = tf.Variable(0, trainable=False)\n",
    "        self.epoch_add_op = self.epoch.assign(self.epoch + 1)\n",
    "        self.momentum = tf.Variable(0.9, trainable=False, dtype=tf.float32)\n",
    "\n",
    "        self.index_queries = self.word2index.lookup(self.queries)  # batch*len\n",
    "        self.index_docs = [self.word2index.lookup(doc) for doc in tf.unstack(self.docs)]\n",
    "\n",
    "        self.embed = tf.get_variable('embed', dtype=tf.float32, initializer=embed)\n",
    "        self.embed_queries = tf.nn.embedding_lookup(self.embed, self.index_queries)\n",
    "        \n",
    "\n",
    "        self.embed_docs = [tf.nn.embedding_lookup(self.embed, index_doc) for index_doc in self.index_docs]\n",
    "\n",
    "\n",
    "\n",
    "        with tf.variable_scope('query_lstm'):\n",
    "            self.cell_q = SimpleLSTMCell(num_lstm_units)\n",
    "        with tf.variable_scope('doc_lstm'):\n",
    "            self.cell_d = SimpleLSTMCell(num_lstm_units)\n",
    "\n",
    "        self.states_q = dynamic_rnn(self.cell_q, self.embed_queries, self.queries_length, dtype=tf.float32,\n",
    "                                         scope=\"simple_lstm_cell_query\")[1][1]  # shape: batch*num_units\n",
    "        self.states_d = [dynamic_rnn(self.cell_d, self.embed_docs[i], self.docs_length[i], dtype=tf.float32,\n",
    "                                            scope=\"simple_lstm_cell_doc\")[1][1] for i in range(neg_num + 1)]  # shape: (neg_num + 1)*batch*num_units\n",
    "        self.queries_norm = tf.sqrt(tf.reduce_sum(tf.square(self.states_q), axis=1))\n",
    "        self.docs_norm = [tf.sqrt(tf.reduce_sum(tf.square(self.states_d[i]), axis=1)) for i in range(neg_num + 1)]\n",
    "        self.prods = [tf.reduce_sum(tf.multiply(self.states_q, self.states_d[i]), axis=1) for i in range(neg_num + 1)]\n",
    "        \n",
    "        self.sims = [(self.prods[i] / (self.queries_norm * self.docs_norm[i])) for i in range(neg_num + 1)]  # shape: (neg_num + 1)*batch\n",
    "        self.sims = tf.convert_to_tensor(self.sims)\n",
    "        self.gamma = tf.Variable(initial_value=1.0, expected_shape=[], dtype=tf.float32)  # scaling factor according to the paper\n",
    "        self.origin_sims = self.sims\n",
    "        self.sims = self.sims * self.gamma\n",
    "        self.prob = tf.nn.softmax(self.sims, dim=0)  # shape: (neg_num + 1)*batch\n",
    "        self.hit_prob = tf.transpose(self.prob[0])\n",
    "\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.hit_prob))\n",
    "\n",
    "        self.params = tf.trainable_variables()\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum, use_nesterov=True)  # use Nesterov's method, according to the paper\n",
    "        gradients = tf.gradients(self.loss, self.params)\n",
    "  \n",
    "        clipped_gradients, self.gradient_norm = tf.clip_by_global_norm(gradients, gradient_clip_threshold)\n",
    "        self.update = opt.apply_gradients(zip(clipped_gradients, self.params), global_step=self.global_step)\n",
    "        #self.update = None\n",
    "        self.saver = tf.train.Saver(write_version=tf.train.SaverDef.V2,\n",
    "                                    max_to_keep=3, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n",
    "\n",
    "    def print_parameters(self):\n",
    "        for item in self.params:\n",
    "            print('%s: %s' % (item.name, item.get_shape()))\n",
    "\n",
    "    def train_step(self, session, queries, docs):\n",
    "        input_feed = {self.queries: queries['texts'],\n",
    "                      self.queries_length: queries['texts_length'],\n",
    "                      self.docs: docs['texts'],\n",
    "                      self.docs_length: docs['texts_length']}\n",
    "\n",
    "        output_feed = [self.loss, self.update, self.states_q, self.states_d, self.queries_norm, self.docs_norm, self.prods, self.sims, self.prob, self.hit_prob]\n",
    "        #output_feed = [self.loss, self.states_q, self.states_d, self.queries_norm, self.docs_norm, self.prods, self.sims, self.prob, self.hit_prob,self.embed_queries,self.embed, self.index_queries]\n",
    "        #gradients = tf.gradients(self.loss, self.params)\n",
    "  \n",
    "        return session.run(output_feed, input_feed)\n",
    "    \n",
    "\n",
    "    def test_step(self, session, queries, docs):\n",
    "        input_feed = {self.queries: queries['texts'],\n",
    "                      self.queries_length: queries['texts_length'],\n",
    "                      self.docs: docs['texts'],\n",
    "                      self.docs_length: docs['texts_length']}\n",
    "                      \n",
    "        output_feed = [self.embed_queries, self.embed, self.states_q]\n",
    "        return session.run(output_feed, input_feed)\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path, fname):\n",
    "    print('Creating dataset...')\n",
    "    data = []\n",
    "    with open('%s/%s' % (path, fname)) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            line = line.strip('\\n')\n",
    "            tokens = line.split()\n",
    "            data.append(tokens)\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_vocab(path, data):\n",
    "    print(\"Creating vocabulary...\")\n",
    "    words = set()\n",
    "    for line in data:\n",
    "        for word in line:\n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "            words.add(word)\n",
    "    words = list(words)\n",
    "    vocab_list = _START_VOCAB + words\n",
    "    FLAGS.symbols = len(vocab_list)\n",
    "\n",
    "    print(\"Loading word vectors...\")\n",
    "    embed = np.random.normal(0.0, np.sqrt(1. / (FLAGS.embed_units)), [len(vocab_list), FLAGS.embed_units])\n",
    "    # debug\n",
    "    # embed = np.array(embed, dtype=np.float32)\n",
    "    # return vocab_list, embed\n",
    "    with open(os.path.join('./data', 'vector.txt')) as fp:\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            info = line.split()\n",
    "            if info[0] not in vocab_list:\n",
    "                continue\n",
    "            embed[vocab_list.index(info[0])] = [float(num) for num in info[1:]]\n",
    "    embed = np.array(embed, dtype=np.float32)\n",
    "    return vocab_list, embed\n",
    "\n",
    "\n",
    "def gen_batch_data(data):\n",
    "    def padding(sent, l):\n",
    "        return sent + ['_PAD'] * (l - len(sent))\n",
    "\n",
    "    max_len = max([len(sentence) for sentence in data])\n",
    "    texts, texts_length = [], []\n",
    "\n",
    "    for item in data:\n",
    "        texts.append(padding(item, max_len)) # padding 0 to make all sentence have the same length\n",
    "        texts_length.append(len(item))\n",
    "\n",
    "    batched_data = {'texts': np.array(texts), 'texts_length': np.array(texts_length, dtype=np.int32)}\n",
    "\n",
    "    return batched_data\n",
    "\n",
    "\n",
    "def train(model, sess, queries, docs):\n",
    "    st, ed, loss = 0, 0, .0 # start and end of the interval. For batch.\n",
    "    lq = len(queries)\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    while ed < lq:\n",
    "        st, ed = ed, ed + FLAGS.batch_size if ed + FLAGS.batch_size < lq else lq\n",
    "        batch_queries = gen_batch_data(queries[st:ed])\n",
    "        batch_docs = gen_batch_data(docs[st*(FLAGS.neg_num + 1):ed*(FLAGS.neg_num + 1)])\n",
    "        texts = []\n",
    "        texts_length = []\n",
    "        for i in range(FLAGS.neg_num + 1):\n",
    "            texts.append(batch_docs['texts'][i::FLAGS.neg_num + 1])\n",
    "            texts_length.append(batch_docs['texts_length'][i::FLAGS.neg_num + 1])\n",
    "        batch_docs['texts'] = texts\n",
    "        batch_docs['texts_length'] = texts_length\n",
    "        outputs = model.train_step(sess, batch_queries, batch_docs)\n",
    "        count += 1\n",
    "        # debug\n",
    "        if math.isnan(outputs[0]):\n",
    "            print('nan detected. ')\n",
    "  \n",
    "        if math.isinf(outputs[0]):\n",
    "            print('inf detected. ')\n",
    "\n",
    "        loss += outputs[0]\n",
    "        ###\n",
    "        #  print('hey ', loss)\n",
    "        \n",
    "\n",
    "\n",
    "    sess.run([model.epoch_add_op])\n",
    "\n",
    "    return loss/count\n",
    "    ###loss / count\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, sess, queries, docs):\n",
    "    st, ed, loss = 0, 0, .0\n",
    "    lq = len(queries)\n",
    "    # debug\n",
    "    # lq = len(queries) // 2 \n",
    "    count = 0\n",
    "    embed_queries_list = list()\n",
    "    state_list = list()\n",
    "    while ed < lq:\n",
    "        st, ed = ed, ed + FLAGS.batch_size if ed + FLAGS.batch_size < lq else lq\n",
    "        batch_queries = gen_batch_data(queries[st:ed])\n",
    "        batch_docs = gen_batch_data(docs[st * (FLAGS.neg_num + 1):ed * (FLAGS.neg_num + 1)])\n",
    "        texts = []\n",
    "        texts_length = []\n",
    "        for i in range(FLAGS.neg_num + 1):\n",
    "            texts.append(batch_docs['texts'][i::FLAGS.neg_num + 1])\n",
    "            texts_length.append(batch_docs['texts_length'][i::FLAGS.neg_num + 1])\n",
    "        batch_docs['texts'] = texts\n",
    "        batch_docs['texts_length'] = texts_length\n",
    "        outputs = model.test_step(sess, batch_queries, batch_docs)\n",
    "        count += 1\n",
    "        embed_queries_list.append(outputs[0])\n",
    "        state_list.append(outputs[2])\n",
    "\n",
    "    return embed_queries_list, outputs[1], state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'read_graph': <absl.flags._flag.BooleanFlag object at 0x10cc39d90>, 'showprefixforinfo': <absl.flags._flag.BooleanFlag object at 0x11e8680d0>, 'stderrthreshold': <absl.logging._StderrthresholdFlag object at 0x11e868050>, 'time_log_path': <absl.flags._flag.Flag object at 0x12290dd90>, 'symbols': <absl.flags._flag.Flag object at 0x10cc39710>, 'op_conversion_fallback_to_while_loop': <absl.flags._flag.BooleanFlag object at 0x1208d8650>, 'test_randomize_ordering_seed': <absl.flags._flag.Flag object at 0x1222720d0>, 'neg_num': <absl.flags._flag.Flag object at 0x12290dd10>, 'data_dir': <absl.flags._flag.Flag object at 0x12290dc10>, 'alsologtostderr': <absl.flags._flag.BooleanFlag object at 0x11e861dd0>, 'embed_units': <absl.flags._flag.Flag object at 0x12290dad0>, 'logtostderr': <absl.flags._flag.BooleanFlag object at 0x11e861cd0>, 'epoch': <absl.flags._flag.Flag object at 0x10cc39750>, 'log_dir': <absl.flags._flag.Flag object at 0x11e861e90>, 'test_tmpdir': <absl.flags._flag.Flag object at 0x12226ad90>, 'is_train': <absl.flags._flag.BooleanFlag object at 0x10cc0d790>, 'run_with_pdb': <absl.flags._flag.BooleanFlag object at 0x11e85a9d0>, 'use_cprofile_for_profiling': <absl.flags._flag.BooleanFlag object at 0x11e868590>, 'xml_output_file': <absl.flags._flag.Flag object at 0x122272b90>, 'batch_size': <absl.flags._flag.Flag object at 0x12290db90>, 'units': <absl.flags._flag.Flag object at 0x12290db10>, 'test_random_seed': <absl.flags._flag.Flag object at 0x12224ffd0>, 'log_parameters': <absl.flags._flag.BooleanFlag object at 0x12290dd50>, 'run_with_profiling': <absl.flags._flag.BooleanFlag object at 0x11e861090>, 'profile_file': <absl.flags._flag.Flag object at 0x11e868550>, 'f': <absl.flags._flag.Flag object at 0x10cc39690>, 'verbosity': <absl.logging._VerbosityFlag object at 0x11e861ed0>, 'train_dir': <absl.flags._flag.Flag object at 0x12290dc90>, 'pdb_post_mortem': <absl.flags._flag.BooleanFlag object at 0x11e861550>, 'only_check_args': <absl.flags._flag.BooleanFlag object at 0x11e8685d0>, 'test_srcdir': <absl.flags._flag.Flag object at 0x12226a510>, 'v': <absl.logging._VerbosityFlag object at 0x11e861ed0>}\nCreating dataset...\nCreating dataset...\nCreating vocabulary...\nLoading word vectors...\nCreating dataset...\nCreating dataset...\nWARNING:tensorflow:From <ipython-input-4-d3af0b47a91f>:61: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From <ipython-input-4-d3af0b47a91f>:73: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\nInstructions for updating:\ndim is deprecated, use axis instead\nembed:0: (2539, 300)\nsimple_lstm_cell_query/simple_lstm_cell/simple_lstm_cell_wi:0: (812, 512)\nsimple_lstm_cell_query/simple_lstm_cell/simple_lstm_cell_bi:0: (512,)\nsimple_lstm_cell_query/simple_lstm_cell/simple_lstm_cell_wo:0: (812, 512)\nsimple_lstm_cell_query/simple_lstm_cell/simple_lstm_cell_bo:0: (512,)\nsimple_lstm_cell_query/simple_lstm_cell/simple_lstm_cell_wc:0: (812, 512)\nsimple_lstm_cell_query/simple_lstm_cell/simple_lstm_cell_bc:0: (512,)\nsimple_lstm_cell_doc/simple_lstm_cell/simple_lstm_cell_wi:0: (812, 512)\nsimple_lstm_cell_doc/simple_lstm_cell/simple_lstm_cell_bi:0: (512,)\nsimple_lstm_cell_doc/simple_lstm_cell/simple_lstm_cell_wo:0: (812, 512)\nsimple_lstm_cell_doc/simple_lstm_cell/simple_lstm_cell_bo:0: (512,)\nsimple_lstm_cell_doc/simple_lstm_cell/simple_lstm_cell_wc:0: (812, 512)\nsimple_lstm_cell_doc/simple_lstm_cell/simple_lstm_cell_bc:0: (512,)\nVariable_4:0: ()\nReading model parameters from ./data\nWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\nINFO:tensorflow:Restoring parameters from ./data/checkpoint-00000021\nbest epoch is: 21\nepoch 21 learning rate 0.0003750000 loss 0.66591346 \nbest epoch is: 22\nepoch 22 learning rate 0.0003750000 loss 0.66578448 \nepoch 23 learning rate 0.0003750000 loss 0.66660738 \nepoch 24 learning rate 0.0003750000 loss 0.66853309 \n"
    }
   ],
   "source": [
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    if FLAGS.is_train:\n",
    "        print(FLAGS.__flags)\n",
    "        data_queries = load_data(FLAGS.data_dir, 'nem_queries.txt')\n",
    "        data_docs = load_data(FLAGS.data_dir, 'nem_docs.txt')\n",
    "        vocab, embed = build_vocab(FLAGS.data_dir, data_queries + data_docs)\n",
    "\n",
    "       \n",
    "        # tweets data\n",
    "        tweets = load_data(FLAGS.data_dir, 'nem_words_for_vec.txt')\n",
    "        docs = load_data(FLAGS.data_dir, 'nem_words_for_vec.txt')\n",
    "        docs = np.repeat(docs, FLAGS.neg_num + 1)\n",
    "        \n",
    "\n",
    "        model = LSTMDSSM(\n",
    "            FLAGS.units,\n",
    "            embed,\n",
    "            FLAGS.neg_num)\n",
    "        if FLAGS.log_parameters:\n",
    "            model.print_parameters()\n",
    "\n",
    "        if tf.train.get_checkpoint_state(FLAGS.train_dir):\n",
    "            print(\"Reading model parameters from %s\" % FLAGS.train_dir)\n",
    "            model.saver.restore(sess, tf.train.latest_checkpoint(FLAGS.train_dir))\n",
    "        else:\n",
    "            print(\"Created model with fresh parameters.\")\n",
    "            tf.global_variables_initializer().run()\n",
    "            op_in = model.word2index.insert(constant_op.constant(vocab),\n",
    "                                              constant_op.constant(list(range(FLAGS.symbols)), dtype=tf.int64))\n",
    "            sess.run(op_in)\n",
    "\n",
    "        # debug\n",
    "        # test_loss = test(model, sess, test_queries, test_docs, test_ground_truths)\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter('%s/log' % FLAGS.train_dir, sess.graph)\n",
    "        pre_losses = [1e18] * 3\n",
    "        best_val_loss = 100\n",
    "\n",
    "        while model.epoch.eval() < FLAGS.epoch:\n",
    "            epoch = model.epoch.eval()\n",
    "            random_idxs = range(len(data_queries))\n",
    "            random.shuffle(random_idxs)\n",
    "            data_queries = [data_queries[i] for i in random_idxs]\n",
    "            data_docs = np.reshape(data_docs, (len(data_queries), -1))\n",
    "            data_docs = [data_docs[i] for i in random_idxs]\n",
    "            data_docs = np.reshape(data_docs, len(data_queries) * (FLAGS.neg_num + 1))\n",
    "           \n",
    "\n",
    "            # loss,embedding, state ,e= train(model, sess, data_queries, data_docs)\n",
    "            loss = train(model, sess, data_queries, data_docs)\n",
    "\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag='loss/train', simple_value=loss)\n",
    "            cur_lr = model.learning_rate.eval()\n",
    "            summary.value.add(tag='lr/train', simple_value=cur_lr)\n",
    "\n",
    "            if(loss <best_val_loss):\n",
    "                best_val_loss = loss\n",
    "                best_epoch=epoch\n",
    "                print(\"best epoch is: %d\" % best_epoch)\n",
    "                model.saver.save(sess, '%s/checkpoint' % FLAGS.train_dir, global_step=model.global_step)\n",
    "                print(\"epoch %d learning rate %.10f loss %.8f \" % (\n",
    "                    epoch, cur_lr, loss))\n",
    "                summary_writer.add_summary(summary, epoch)\n",
    "            else:\n",
    "                print(\"epoch %d learning rate %.10f loss %.8f \" % (\n",
    "                    epoch, cur_lr, loss))\n",
    "            \n",
    "            if loss > max(pre_losses):\n",
    "                op = tf.assign(model.learning_rate, cur_lr * 0.5)\n",
    "                sess.run(op)\n",
    "            pre_losses = pre_losses[1:] + [loss]\n",
    "        #with open(os.path.join(FLAGS.train_dir, FLAGS.time_log_path), 'a') as fp:\n",
    "            #fp.writelines(['total training time: %f\\n' % total_train_time, 'last loss: %.8f' % loss])\n",
    "\n",
    "        embed_queries, embed, state_q = test(model, sess, tweets, docs)\n",
    "        #print(embed)\n",
    "\n",
    "\n",
    "\n",
    "#with open(os.path.join('./data', 'word2vec.txt'), 'w') as f:\n",
    "#        f.writelines(embeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = [state for batch in state_q for state in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the csv file in 'w+' mode \n",
    "file = open('nem_state_q.csv', 'w+') \n",
    "  \n",
    "# writing the data into the file \n",
    "with file:     \n",
    "    write = csv.writer(file) \n",
    "    write.writerows(state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\nwith open('state_q.csv') as f:\\n    reader = csv.reader(f)\\n    state = list(reader)\\n\""
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open('state_q.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    state = list(reader)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.17 64-bit",
   "language": "python",
   "name": "python271764bitedafd2d1ef3443068e78f0305acdf7ee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.17-candidate"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}